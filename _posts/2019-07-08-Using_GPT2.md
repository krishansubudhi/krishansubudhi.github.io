---
author: krishan
layout: post
categories: deeplearning
title: Using GPT2
---

GPT2 model have higher memory requirement when compared to BERT models. Using FP16 I was able to load and train on GPT2 models.

[FP16 documentation](https://devblogs.nvidia.com/apex-pytorch-easy-mixed-precision-training/)

```python

from transformers import *
import sys,logging
logging.root.handlers = []
logging.basicConfig(level="INFO", format = '%(asctime)s:%(levelname)s: %(message)s' ,stream = sys.stdout)
logger = logging.getLogger(__name__)
logger.info('hello')
```

    I1007 22:23:49.255996 140428090640128 file_utils.py:39] PyTorch version 1.1.0 available.
    I1007 22:23:49.494452 140428090640128 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .


    2019-10-07 22:23:49,498:INFO: hello



```python
import transformers
import torch
from transformers import GPT2Model

def check_memory():
    logger.info('GPU memory: %.1f' % (torch.cuda.memory_allocated() // 1024 ** 2))
```


```python
device = torch.device('cuda')
torch.cuda.empty_cache()
check_memory()

model = GPT2LMHeadModel.from_pretrained('gpt2-medium')

model.half()
gpu_model = model.to(device)

```

    2019-10-07 22:23:49,531:INFO: GPU memory: 0.0
    2019-10-07 22:23:49,706:INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-config.json from cache at /home/enlr/.cache/torch/transformers/98aa65385e18b0efd17acd8bf64dcdf21406bb0c99c801c2d3c9f6bfd1f48f29.5f9150c569dadadaa1e66830d29254aa5cf43f8ccd76dc0c81e0102c67032367
    2019-10-07 22:23:49,707:INFO: Model config {
      "attn_pdrop": 0.1,
      "embd_pdrop": 0.1,
      "finetuning_task": null,
      "initializer_range": 0.02,
      "layer_norm_epsilon": 1e-05,
      "n_ctx": 1024,
      "n_embd": 1024,
      "n_head": 16,
      "n_layer": 24,
      "n_positions": 1024,
      "n_special": 0,
      "num_labels": 1,
      "output_attentions": false,
      "output_hidden_states": false,
      "predict_special_tokens": true,
      "pruned_heads": {},
      "resid_pdrop": 0.1,
      "summary_activation": null,
      "summary_first_dropout": 0.1,
      "summary_proj_to_labels": true,
      "summary_type": "cls_index",
      "summary_use_proj": true,
      "torchscript": false,
      "use_bfloat16": false,
      "vocab_size": 50257
    }
    
    2019-10-07 22:23:49,881:INFO: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin from cache at /home/enlr/.cache/torch/transformers/4b337a4f3b7d3e1518f799e238af607498c02938a3390152aaec7d4dabca5a02.8769029be4f66a5ae1055eefdd1d11621b901d510654266b8681719fff492d6e



```python
def run_gpt(batch_size = 1, seq_len = 1024):
    x = torch.randint(low =1000, high = 30000 , size = (batch_size,seq_len))
    x = x.to(device)
    check_memory()
    loss, yhat,_= gpu_model.forward(x,labels=x)

    print(loss, yhat.size())

    optimizer = torch.optim.Adam(gpu_model.parameters())

    loss.backward()
    optimizer.step()

    optimizer.zero_grad()
```


```python
run_gpt(seq_len = 1024)
```

    2019-10-07 22:24:28,790:INFO: GPU memory: 724.0
    tensor(11.6250, device='cuda:0', dtype=torch.float16,
           grad_fn=<NllLossBackward>) torch.Size([1, 1024, 50257])

